import Foundation

// æ®µè½ç±»å‹æšä¸¾
enum SegmentType {
    case silence       // é™éŸ³
    case speech        // è¯­éŸ³
    case music         // éŸ³ä¹
    case noise         // å™ªéŸ³
    case unknown       // æœªçŸ¥
}

// æ®µè½ç»“æ„
struct Segment {
    let startTime: Double
    let endTime: Double
    let type: SegmentType
    let confidence: Float  // ç½®ä¿¡åº¦ 0-1
    let features: AcousticFeatures?
    
    var duration: Double { endTime - startTime }
    
    var description: String {
        let typeStr: String
        switch type {
        case .silence: typeStr = "é™éŸ³"
        case .speech: typeStr = "è¯­éŸ³"
        case .music: typeStr = "éŸ³ä¹"
        case .noise: typeStr = "å™ªéŸ³"
        case .unknown: typeStr = "æœªçŸ¥"
        }
        return "\(typeStr) [\(String(format: "%.2f", startTime))s - \(String(format: "%.2f", endTime))s] (ç½®ä¿¡åº¦: \(String(format: "%.0f", confidence*100))%)"
    }
}

final class SegmentDetector {
    private let features: [AcousticFeatures]
    private let hopSize: Int = 768  // ä¸AcousticFeatureExtractorä¿æŒä¸€è‡´ï¼ˆçº¦17.4ms @ 44100Hzï¼Œ25%é‡å ï¼‰
    private let sampleRate: Double = 44100  // é»˜è®¤é‡‡æ ·ç‡
    
    var segments: [Segment] = []
    
    init(features: [AcousticFeatures]) {
        self.features = features
    }
    
    // æ£€æµ‹æ‰€æœ‰ç±»å‹çš„æ®µè½
    func detectSegments() {
        segments.removeAll()
        
        // 1. æ£€æµ‹é™éŸ³æ®µ
        detectSilence()
        
        // 2. åœ¨éé™éŸ³æ®µæ£€æµ‹éŸ³ä¹å’Œè¯­éŸ³
        detectMusicAndSpeech()
        
        print("âœ“ æ®µè½æ£€æµ‹å®Œæˆ: å…±\(segments.count)ä¸ªæ®µè½")
    }
    
    // æ£€æµ‹é™éŸ³æ®µï¼ˆåŸºäºèƒ½é‡é˜ˆå€¼ï¼‰
    private func detectSilence() {
        let energyThreshold: Float = -40.0  // dB
        let minSilenceDuration: Double = 0.5  // ç§’ï¼ˆæœ€å°é™éŸ³æ—¶é•¿ï¼‰
        
        var inSilence = false
        var silenceStartIdx = 0
        
        for (idx, feature) in features.enumerated() {
            let isSilent = feature.energy < energyThreshold
            
            if isSilent && !inSilence {
                // è¿›å…¥é™éŸ³
                inSilence = true
                silenceStartIdx = idx
            } else if !isSilent && inSilence {
                // ç¦»å¼€é™éŸ³
                let duration = Double(idx - silenceStartIdx) * Double(hopSize) / sampleRate
                if duration >= minSilenceDuration {
                    let startTime = Double(silenceStartIdx) * Double(hopSize) / sampleRate
                    let endTime = Double(idx) * Double(hopSize) / sampleRate
                    let segment = Segment(
                        startTime: startTime,
                        endTime: endTime,
                        type: .silence,
                        confidence: 0.9,
                        features: nil
                    )
                    segments.append(segment)
                }
                inSilence = false
            }
        }
        
        // å¤„ç†æœ«å°¾çš„é™éŸ³
        if inSilence {
            let duration = Double(features.count - silenceStartIdx) * Double(hopSize) / sampleRate
            if duration >= minSilenceDuration {
                let startTime = Double(silenceStartIdx) * Double(hopSize) / sampleRate
                let endTime = Double(features.count) * Double(hopSize) / sampleRate
                let segment = Segment(
                    startTime: startTime,
                    endTime: endTime,
                    type: .silence,
                    confidence: 0.9,
                    features: nil
                )
                segments.append(segment)
            }
        }
        
        print("ğŸ“ æ£€æµ‹åˆ°\(segments.count)ä¸ªé™éŸ³æ®µ")
    }
    
    // æ£€æµ‹éŸ³ä¹å’Œè¯­éŸ³ï¼ˆåŸºäºç‰¹å¾å¯¹æ¯”ï¼‰
    private func detectMusicAndSpeech() {
        let energyThreshold: Float = -40.0 // èƒ½é‡é˜ˆå€¼
        let minSegmentDuration: Double = 0.3  // æœ€å°æ®µè½æ—¶é•¿
        
        var currentType: SegmentType? = nil
        var segmentStartIdx = 0
        var typeConfidences: [Float] = []
        
        for (idx, feature) in features.enumerated() {
            // è·³è¿‡é™éŸ³
            if feature.energy < energyThreshold {
                if currentType != nil {
                    addSegmentIfValid(
                        type: currentType!,
                        startIdx: segmentStartIdx,
                        endIdx: idx,
                        confidences: typeConfidences,
                        minDuration: minSegmentDuration
                    )
                    currentType = nil
                    typeConfidences.removeAll()
                }
                continue
            }
            
            // åˆ¤æ–­å½“å‰å¸§çš„ç±»å‹ï¼ˆæ¯100å¸§æ‰“å°ä¸€æ¬¡ç‰¹å¾ç”¨äºè°ƒè¯•ï¼‰
            let frameType = detectFrameType(feature: feature)
            
            if idx % 100 == 0 {
                let mfccStr = feature.mfccValues.isEmpty ? "æ— " : "[\(feature.mfccValues.prefix(3).map { String(format: "%.2f", $0) }.joined(separator: ","))]"
                print("ğŸ” [å¸§\(idx)] ç±»å‹=\(frameType == .speech ? "è¯­éŸ³" : "éŸ³ä¹"), ZCR=\(String(format: "%.4f", feature.zcr)), èƒ½é‡=\(String(format: "%.1f", feature.energy))dB, MFCC=\(mfccStr)")
            }
            
            if frameType == currentType {
                // ç»§ç»­å½“å‰ç±»å‹
                typeConfidences.append(feature.energy)
            } else {
                // ç±»å‹æ”¹å˜ï¼Œä¿å­˜å‰ä¸€ä¸ªæ®µè½
                if currentType != nil {
                    addSegmentIfValid(
                        type: currentType!,
                        startIdx: segmentStartIdx,
                        endIdx: idx,
                        confidences: typeConfidences,
                        minDuration: minSegmentDuration
                    )
                }
                
                currentType = frameType
                segmentStartIdx = idx
                typeConfidences = [feature.energy]
            }
        }
        
        // å¤„ç†æœ«å°¾æ®µè½
        if currentType != nil {
            addSegmentIfValid(
                type: currentType!,
                startIdx: segmentStartIdx,
                endIdx: features.count,
                confidences: typeConfidences,
                minDuration: minSegmentDuration
            )
        }
    }
    
    // æ·»åŠ æœ‰æ•ˆçš„æ®µè½ï¼ˆè¶…è¿‡æœ€å°æ—¶é•¿ï¼‰
    private func addSegmentIfValid(
        type: SegmentType,
        startIdx: Int,
        endIdx: Int,
        confidences: [Float],
        minDuration: Double
    ) {
        let duration = Double(endIdx - startIdx) * Double(hopSize) / sampleRate
        guard duration >= minDuration else { return }
        
        let startTime = Double(startIdx) * Double(hopSize) / sampleRate
        let endTime = Double(endIdx) * Double(hopSize) / sampleRate
        
        // è®¡ç®—ç½®ä¿¡åº¦ï¼ˆåŸºäºèƒ½é‡ç¨³å®šæ€§ï¼‰
        let avgEnergy = confidences.reduce(0, +) / Float(confidences.count)
        let variance = confidences.map { pow($0 - avgEnergy, 2) }.reduce(0, +) / Float(confidences.count)
        let confidence = max(0.5, 1.0 - sqrt(variance) / 50.0)
        
        let segment = Segment(
            startTime: startTime,
            endTime: endTime,
            type: type,
            confidence: confidence,
            features: features.first { abs($0.timestamp - startTime) < 0.1 }
        )
        
        segments.append(segment)
    }
    
    // è¯†åˆ«å•å¸§çš„ç±»å‹
    // ç­–ç•¥ï¼šç»¼åˆä½¿ç”¨ZCRã€MFCCå’Œèƒ½é‡ç‰¹å¾æ¥åˆ¤æ–­
    private func detectFrameType(feature: AcousticFeatures) -> SegmentType {
        // ç‰¹å¾åˆ†æï¼š
        // - è¯­éŸ³ï¼šZCRè¾ƒé«˜(0.1-0.4), MFCCå‰å‡ ä¸ªç³»æ•°å˜åŒ–å¤§, èƒ½é‡æ³¢åŠ¨å¤§
        // - éŸ³ä¹ï¼šZCRè¾ƒä½(0.03-0.15), MFCCç³»æ•°è¾ƒç¨³å®š, èƒ½é‡è¾ƒå¹³ç¨³
        
        var musicScore: Float = 0.0  // éŸ³ä¹å¾—åˆ†
        var speechScore: Float = 0.0  // è¯­éŸ³å¾—åˆ†
        
        // 1. ZCRç‰¹å¾ï¼ˆæƒé‡ï¼šé«˜ï¼‰
        if feature.zcr > 0 {
            if feature.zcr < 0.05 {
                musicScore += 3.0  // ZCRæä½ï¼Œå¼ºçƒˆå€¾å‘éŸ³ä¹
            } else if feature.zcr < 0.10 {
                musicScore += 1.0  // ZCRè¾ƒä½ï¼Œè½»å¾®å€¾å‘éŸ³ä¹
            } else if feature.zcr > 0.15 {
                speechScore += 3.0  // ZCRé«˜ï¼Œå¼ºçƒˆå€¾å‘è¯­éŸ³
            } else {
                speechScore += 1.5  // ZCRä¸­ç­‰ï¼Œå€¾å‘è¯­éŸ³
            }
        }
        
        // 2. MFCCç‰¹å¾ï¼ˆæƒé‡ï¼šé«˜ï¼‰
        if !feature.mfccValues.isEmpty && feature.mfccValues.count >= 4 {
            // MFCC[0]æ˜¯èƒ½é‡ï¼Œ[1-3]åæ˜ é¢‘è°±å½¢çŠ¶
            let mfcc1 = abs(feature.mfccValues[1])  // é€šå¸¸è¯­éŸ³çš„å˜åŒ–æ›´å¤§
            let mfcc2 = abs(feature.mfccValues[2])  
            let mfcc3 = abs(feature.mfccValues[3])
            
            // è®¡ç®—MFCCçš„"å¹³æ»‘åº¦"ï¼ˆå‰å‡ ä¸ªç³»æ•°çš„ç»å¯¹å€¼ï¼‰
            let mfccSmoothness = (mfcc1 + mfcc2 + mfcc3) / 3.0
            
            // éŸ³ä¹ï¼šMFCCç³»æ•°é€šå¸¸è¾ƒå°ä¸”ç¨³å®šï¼ˆå¹³æ»‘åº¦é«˜ï¼‰
            // è¯­éŸ³ï¼šMFCCç³»æ•°å˜åŒ–è¾ƒå¤§ï¼ˆå¹³æ»‘åº¦ä½ï¼‰
            if mfccSmoothness < 2.0 {
                musicScore += 2.0  // MFCCå¾ˆå¹³æ»‘ï¼Œå€¾å‘éŸ³ä¹
            } else if mfccSmoothness > 5.0 {
                speechScore += 2.0  // MFCCå˜åŒ–å¤§ï¼Œå€¾å‘è¯­éŸ³
            }
            
            // MFCCèƒ½é‡åˆ†å¸ƒç‰¹å¾
            // éŸ³ä¹é€šå¸¸å‰å‡ ä¸ªMFCCç³»æ•°ï¼ˆé™¤äº†èƒ½é‡ï¼‰ç›¸å¯¹å‡åŒ€
            if feature.mfccValues.count > 3 {
                let mfccSlice = Array(feature.mfccValues[1...3])
                if let maxVal = mfccSlice.max(), let minVal = mfccSlice.min() {
                    let mfccRange = maxVal - minVal
                    if mfccRange < 3.0 {
                        musicScore += 1.0  // MFCCç³»æ•°èŒƒå›´å°ï¼Œæ›´å¯èƒ½æ˜¯éŸ³ä¹
                    } else if mfccRange > 8.0 {
                        speechScore += 1.0  // MFCCç³»æ•°èŒƒå›´å¤§ï¼Œæ›´å¯èƒ½æ˜¯è¯­éŸ³
                    }
                }
            }
        }
        
        // 3. èƒ½é‡ç‰¹å¾ï¼ˆæƒé‡ï¼šä¸­ï¼‰
        // éŸ³ä¹é€šå¸¸èƒ½é‡æ›´ç¨³å®šï¼Œè¯­éŸ³èƒ½é‡æ³¢åŠ¨æ›´å¤§
        // è¿™é‡Œæˆ‘ä»¬ä¸»è¦ç”¨èƒ½é‡æ¥æ’é™¤é™éŸ³ï¼Œå·²ç»åœ¨ä¸Šé¢å¤„ç†äº†
        
        // 4. è°±è´¨å¿ƒç‰¹å¾ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if feature.spectralCentroid > 0 {
            if feature.spectralCentroid < 2000 && feature.zcr < 0.08 {
                musicScore += 2.0  // ä½é¢‘ + ä½ZCR â†’ éŸ³ä¹
            } else if feature.spectralCentroid > 4000 {
                speechScore += 1.0  // é«˜é¢‘ â†’ è¯­éŸ³
            }
        }
        
        // ç»¼åˆåˆ¤æ–­ï¼ˆé‡‡ç”¨ä¿å®ˆç­–ç•¥ï¼šéŸ³ä¹éœ€è¦æ˜ç¡®çš„è¯æ®ï¼‰
        // å¦‚æœmusicScoreæ˜¾è‘—é«˜äºspeechScoreï¼Œæ‰åˆ¤ä¸ºéŸ³ä¹
        if musicScore > speechScore + 2.0 {
            return .music
        } else {
            // é»˜è®¤åˆ¤ä¸ºè¯­éŸ³ï¼ˆæ’­å®¢åœºæ™¯ï¼‰
            return .speech
        }
    }
}
