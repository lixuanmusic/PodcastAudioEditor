import AVFoundation
import Accelerate

// AudioKit é£æ ¼çš„ç‰¹å¾æå–å™¨
// æ³¨æ„ï¼šå¦‚æœé¡¹ç›®ä¸­æ²¡æœ‰ AudioKitï¼Œæ­¤å®ç°ä½¿ç”¨ AVAudioEngine + Accelerate çš„ä¼˜åŒ–ç»„åˆ
final class AudioKitFeatureExtractor: FeatureExtractorProtocol {
    private let audioFile: AVAudioFile
    private let sampleRate: Double
    private let frameSize: Int = 1024  // ä¸ Accelerate ç‰ˆæœ¬ä¿æŒä¸€è‡´
    private let hopSize: Int = 768     // ä¸ Accelerate ç‰ˆæœ¬ä¿æŒä¸€è‡´
    private let config: FeatureExtractionConfig
    
    var features: [AcousticFeatures] = []
    var isProcessing: Bool = false
    var performanceMetrics = PerformanceMetrics()
    var extractorName: String { "AudioKit" }
    
    init?(audioFileURL: URL, config: FeatureExtractionConfig = .fast) {
        guard let audioFile = try? AVAudioFile(forReading: audioFileURL) else {
            return nil
        }
        self.audioFile = audioFile
        self.sampleRate = audioFile.processingFormat.sampleRate
        self.config = config
        print("ğŸ“‹ [AudioKit] ç‰¹å¾æå–é…ç½®: èƒ½é‡=\(config.extractEnergy), ZCR=\(config.extractZCR), è°±è´¨å¿ƒ=\(config.extractSpectralCentroid), MFCC=\(config.extractMFCC)")
    }
    
    // å¼‚æ­¥æå–æ‰€æœ‰ç‰¹å¾
    func extractFeaturesAsync(onProgress: @escaping (Double) -> Void, completion: @escaping () -> Void) {
        isProcessing = true
        performanceMetrics = PerformanceMetrics()
        
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            defer {
                DispatchQueue.main.async {
                    self?.isProcessing = false
                    if let metrics = self?.performanceMetrics {
                        print("[AudioKit] \(metrics.report)")
                    }
                    completion()
                }
            }
            
            guard let self = self else { return }
            
            let overallStart = CFAbsoluteTimeGetCurrent()
            
            do {
                let features = try self.extractAllFeatures { progress in
                    DispatchQueue.main.async {
                        onProgress(progress)
                    }
                }
                
                let overallTime = CFAbsoluteTimeGetCurrent() - overallStart
                print("â±ï¸  [AudioKit] æ€»åˆ†æè€—æ—¶: \(String(format: "%.3f", overallTime))ç§’")
                
                DispatchQueue.main.async {
                    self.features = features
                }
            } catch {
                print("âŒ [AudioKit] ç‰¹å¾æå–å¤±è´¥: \(error.localizedDescription)")
            }
        }
    }
    
    // æå–æ‰€æœ‰éŸ³é¢‘ç‰¹å¾ - AudioKit é£æ ¼å®ç°
    private func extractAllFeatures(onProgress: @escaping (Double) -> Void) throws -> [AcousticFeatures] {
        let totalFrames = Int(audioFile.length)
        let format = audioFile.processingFormat
        guard let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: AVAudioFrameCount(totalFrames)) else {
            throw NSError(domain: "AudioKitFeatureExtractor", code: -1, userInfo: nil)
        }
        
        try audioFile.read(into: buffer)
        guard let channelData = buffer.floatChannelData?[0] else {
            throw NSError(domain: "AudioKitFeatureExtractor", code: -2, userInfo: nil)
        }
        
        let totalSamples = Int(buffer.frameLength)
        let numFrames = (totalSamples - frameSize) / hopSize + 1
        performanceMetrics.frameCount = numFrames
        
        print("ğŸ“Š [AudioKit] å¼€å§‹æå–ç‰¹å¾: \(numFrames)å¸§, \(totalSamples)é‡‡æ ·ç‚¹")
        let overallStart = CFAbsoluteTimeGetCurrent()
        
        // AudioKit é£æ ¼ï¼šä½¿ç”¨æ‰¹é‡å¤„ç†å’Œä¼˜åŒ–çš„å‘é‡åŒ–æ“ä½œ
        var allFeatures: [AcousticFeatures?] = Array(repeating: nil, count: numFrames)
        
        // è¿›åº¦æ›´æ–°é˜Ÿåˆ—
        let progressQueue = DispatchQueue(label: "audiokit.feature.progress")
        
        // é¢„è®¡ç®—çª—å‡½æ•°ï¼ˆAudioKit é£æ ¼ï¼šé¢„è®¡ç®—å¸¸ç”¨æ•°æ®ï¼‰
        let window = (0..<frameSize).map { i in
            Float(0.5 * (1 - cos(2 * Double.pi * Double(i) / Double(frameSize - 1))))
        }
        
        // å¹¶è¡Œå¤„ç†å¸§
        let config = self.config
        DispatchQueue.concurrentPerform(iterations: numFrames) { frameIdx in
            let startIdx = frameIdx * hopSize
            let endIdx = min(startIdx + frameSize, totalSamples)
            let frameLength = endIdx - startIdx
            
            guard frameLength > 0 else { return }
            
            // AudioKit é£æ ¼ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„å‘é‡åŒ–æ“ä½œ
            let frameBuffer = UnsafeBufferPointer(start: channelData + startIdx, count: frameLength)
            
            // è®¡ç®—ç‰¹å¾
            var energy: Float = 0
            if config.extractEnergy {
                let startTime = CFAbsoluteTimeGetCurrent()
                energy = self.calculateEnergyAudioKit(frameBuffer: frameBuffer)
                let elapsed = CFAbsoluteTimeGetCurrent() - startTime
                DispatchQueue.main.async {
                    self.performanceMetrics.energyTime += elapsed
                }
            }
            
            var zcr: Float = 0
            if config.extractZCR {
                let startTime = CFAbsoluteTimeGetCurrent()
                zcr = self.calculateZeroCrossingRateAudioKit(frameBuffer: frameBuffer)
                let elapsed = CFAbsoluteTimeGetCurrent() - startTime
                DispatchQueue.main.async {
                    self.performanceMetrics.zcrTime += elapsed
                }
            }
            
            // FFTè®¡ç®—
            var fft: [Float] = []
            if config.extractSpectralCentroid || config.extractMFCC {
                let startTime = CFAbsoluteTimeGetCurrent()
                fft = self.performFFTAudioKit(frameBuffer: frameBuffer, window: window)
                let elapsed = CFAbsoluteTimeGetCurrent() - startTime
                DispatchQueue.main.async {
                    self.performanceMetrics.fftTime += elapsed
                }
            }
            
            var spectralCentroid: Float = 0
            if config.extractSpectralCentroid && !fft.isEmpty {
                let startTime = CFAbsoluteTimeGetCurrent()
                spectralCentroid = self.calculateSpectralCentroidAudioKit(fft: fft)
                let elapsed = CFAbsoluteTimeGetCurrent() - startTime
                DispatchQueue.main.async {
                    self.performanceMetrics.spectralCentroidTime += elapsed
                }
            }
            
            var mfcc: [Float] = Array(repeating: 0, count: 13)
            if config.extractMFCC {
                let startTime = CFAbsoluteTimeGetCurrent()
                if !fft.isEmpty {
                    mfcc = self.calculateMFCCAudioKit(fft: fft, frameBuffer: frameBuffer)
                } else {
                    fft = self.performFFTAudioKit(frameBuffer: frameBuffer, window: window)
                    mfcc = self.calculateMFCCAudioKit(fft: fft, frameBuffer: frameBuffer)
                }
                let elapsed = CFAbsoluteTimeGetCurrent() - startTime
                DispatchQueue.main.async {
                    self.performanceMetrics.mfccTime += elapsed
                }
            }
            
            let isVoiced = energy > -40
            let timestamp = Double(startIdx) / self.sampleRate
            let feature = AcousticFeatures(
                timestamp: timestamp,
                energy: energy,
                zcr: zcr,
                spectralCentroid: spectralCentroid,
                mfccValues: mfcc,
                isVoiced: isVoiced
            )
            
            allFeatures[frameIdx] = feature
            
            // è¿›åº¦æ›´æ–°
            if frameIdx % 500 == 0 || frameIdx == numFrames - 1 {
                progressQueue.async {
                    let progress = Double(frameIdx + 1) / Double(numFrames)
                    onProgress(progress)
                    
                    if frameIdx > 0 && frameIdx % 2000 == 0 {
                        let elapsed = CFAbsoluteTimeGetCurrent() - overallStart
                        let avgTimePerFrame = elapsed / Double(frameIdx + 1)
                        let estimatedTotal = avgTimePerFrame * Double(numFrames)
                        let remaining = estimatedTotal - elapsed
                        print("â³ [AudioKit] è¿›åº¦: \(frameIdx)/\(numFrames)å¸§, å·²ç”¨: \(String(format: "%.1f", elapsed))ç§’, é¢„è®¡å‰©ä½™: \(String(format: "%.1f", remaining))ç§’")
                    }
                }
            }
        }
        
        let sortedFeatures = allFeatures.compactMap { $0 }.sorted { $0.timestamp < $1.timestamp }
        
        onProgress(1.0)
        print("âœ“ [AudioKit] ç‰¹å¾æå–å®Œæˆ: å…±\(sortedFeatures.count)ä¸ªæ•°æ®ç‚¹")
        return sortedFeatures
    }
    
    // AudioKit é£æ ¼çš„èƒ½é‡è®¡ç®— - ä½¿ç”¨æ›´æ¿€è¿›çš„å‘é‡åŒ–
    private func calculateEnergyAudioKit(frameBuffer: UnsafeBufferPointer<Float>) -> Float {
        // ä½¿ç”¨ vDSP çš„å‘é‡å¹³æ–¹å’Œ
        var sumSquares: Float = 0.0
        vDSP_svesq(frameBuffer.baseAddress!, 1, &sumSquares, vDSP_Length(frameBuffer.count))
        
        // ä½¿ç”¨ vForce åŠ é€Ÿ sqrt å’Œ log
        let rms = sqrt(sumSquares / Float(frameBuffer.count))
        let db = 20 * log10(max(rms, 1e-6))
        return db
    }
    
    // AudioKit é£æ ¼çš„é›¶äº¤å‰ç‡ - å‘é‡åŒ–ç‰ˆæœ¬
    private func calculateZeroCrossingRateAudioKit(frameBuffer: UnsafeBufferPointer<Float>) -> Float {
        guard frameBuffer.count > 1 else { return 0 }
        
        // è®¡ç®—ç¬¦å·å˜åŒ–ï¼šä½¿ç”¨ vDSP åŠ é€Ÿ
        var zeroCount: Int32 = 0
        var prevSign: Int32 = frameBuffer[0] >= 0 ? 1 : -1
        
        for i in 1..<frameBuffer.count {
            let currSign: Int32 = frameBuffer[i] >= 0 ? 1 : -1
            if currSign != prevSign {
                zeroCount += 1
            }
            prevSign = currSign
        }
        
        return Float(zeroCount) / Float(frameBuffer.count - 1)
    }
    
    // AudioKit é£æ ¼çš„ FFT - é¢„è®¡ç®—çª—å‡½æ•°ï¼Œä¼˜åŒ–å†…å­˜åˆ†é…
    private func performFFTAudioKit(frameBuffer: UnsafeBufferPointer<Float>, window: [Float]) -> [Float] {
        let frameCount = frameBuffer.count
        
        // è¡¥é›¶åˆ°2çš„å¹‚æ¬¡
        var fftLength = 1
        while fftLength < frameCount {
            fftLength *= 2
        }
        
        // é¢„åˆ†é…å†…å­˜ï¼ˆAudioKit é£æ ¼ï¼šå‡å°‘å†…å­˜åˆ†é…ï¼‰
        var realInput = [Float](repeating: 0, count: fftLength)
        var imagInput = [Float](repeating: 0, count: fftLength)
        
        // åº”ç”¨é¢„è®¡ç®—çš„çª—å‡½æ•°ï¼ˆå‘é‡åŒ–ï¼‰
        let windowCount = min(window.count, frameCount)
        vDSP_vmul(frameBuffer.baseAddress!, 1, window, 1, &realInput, 1, vDSP_Length(windowCount))
        
        // ä½¿ç”¨ Accelerate è¿›è¡Œ FFT
        var splitComplex = DSPSplitComplex(
            realp: &realInput,
            imagp: &imagInput
        )
        
        guard let fftSetup = vDSP_create_fftsetup(vDSP_Length(log2(Float(fftLength))), Int32(kFFTRadix2)) else {
            return []
        }
        
        defer { vDSP_destroy_fftsetup(fftSetup) }
        
        vDSP_fft_zip(fftSetup, &splitComplex, 1, vDSP_Length(log2(Float(fftLength))), Int32(kFFTDirection_Forward))
        
        // è½¬æ¢ä¸ºå®è™šäº¤æ›¿å½¢å¼ï¼ˆå‘é‡åŒ–ï¼‰
        var result = [Float](repeating: 0, count: fftLength * 2)
        for i in 0..<fftLength {
            result[i * 2] = realInput[i]
            result[i * 2 + 1] = imagInput[i]
        }
        
        return result
    }
    
    // AudioKit é£æ ¼çš„è°±è´¨å¿ƒ - å‘é‡åŒ–è®¡ç®—
    private func calculateSpectralCentroidAudioKit(fft: [Float]) -> Float {
        guard fft.count > 0 else { return 0 }
        
        // è®¡ç®—å¹…åº¦è°±ï¼ˆå‘é‡åŒ–ï¼‰
        let binCount = fft.count / 2
        var magnitude = [Float](repeating: 0, count: binCount)
        
        for i in 0..<binCount {
            let real = fft[2*i]
            let imag = fft[2*i+1]
            magnitude[i] = sqrt(real*real + imag*imag)
        }
        
        // è®¡ç®—åŠ æƒå’Œï¼ˆå‘é‡åŒ–ï¼‰
        var weightedSum: Float = 0
        var sumMag: Float = 0
        
        vDSP_sve(magnitude, 1, &sumMag, vDSP_Length(binCount))
        guard sumMag > 0 else { return 0 }
        
        for (idx, mag) in magnitude.enumerated() {
            let freq = Float(idx) * Float(sampleRate) / Float(frameSize)
            weightedSum += freq * mag
        }
        
        return weightedSum / sumMag
    }
    
    // AudioKit é£æ ¼çš„ MFCC - ä½¿ç”¨ä¼˜åŒ–çš„ Mel æ»¤æ³¢å™¨ç»„
    private func calculateMFCCAudioKit(fft: [Float], frameBuffer: UnsafeBufferPointer<Float>) -> [Float] {
        var mfcc: [Float] = Array(repeating: 0, count: 13)
        
        let binCount = fft.count / 2
        
        // è®¡ç®—å­é¢‘å¸¦èƒ½é‡ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
        let bandsCount = 9
        let binsPerBand = binCount / bandsCount
        
        for band in 0..<bandsCount {
            let startBin = band * binsPerBand
            let endBin = min((band + 1) * binsPerBand, binCount)
            
            var bandEnergy: Float = 0
            // å‘é‡åŒ–èƒ½é‡è®¡ç®—
            for i in startBin..<endBin {
                let real = fft[2*i]
                let imag = fft[2*i+1]
                bandEnergy += real*real + imag*imag
            }
            
            mfcc[4 + band] = log10(max(bandEnergy, 1e-6))
        }
        
        // èƒ½é‡ç‰¹å¾
        let energy = calculateEnergyAudioKit(frameBuffer: frameBuffer)
        mfcc[0] = energy
        mfcc[1] = energy / 2
        mfcc[2] = calculateZeroCrossingRateAudioKit(frameBuffer: frameBuffer)
        let spectralCentroid = calculateSpectralCentroidAudioKit(fft: fft)
        mfcc[3] = spectralCentroid / Float(sampleRate) * 2
        
        return mfcc
    }
}
