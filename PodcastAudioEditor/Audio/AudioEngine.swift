import AVFoundation
import Foundation
import Combine

// æ³¢å½¢å¤„ç†é…ç½®ï¼ˆå‚è€ƒ Miniwaveï¼‰
struct WaveformProcessingConfig {
    let numberOfSlices: Int
    let targetDataPoints: Int
    let smallFileThreshold: Int
    let useParallelProcessing: Bool
    
    init(numberOfSlices: Int = 10,
         targetDataPoints: Int = 2000,
         smallFileThreshold: Int = 100000,
         useParallelProcessing: Bool = true) {
        self.numberOfSlices = numberOfSlices
        self.targetDataPoints = targetDataPoints
        self.smallFileThreshold = smallFileThreshold
        self.useParallelProcessing = useParallelProcessing
    }
}

final class AudioEngine: ObservableObject {
    static let shared = AudioEngine()
    
    // ç»Ÿä¸€çš„éŸ³é¢‘å¼•æ“
    private var audioEngine: AVAudioEngine?
    private var playerNode: AVAudioPlayerNode?
    private var audioFile: AVAudioFile?
    private var format: AVAudioFormat?
    
    // éŸ³é‡å¹³è¡¡æ•ˆæœå™¨
    private var eqUnit: AVAudioUnitEQ?
    private var gains: [Float] = []
    private var hopSize: Int = 768
    private var sampleRate: Double = 44100
    
    @Published var isPlaying: Bool = false
    @Published var currentTime: TimeInterval = 0
    @Published var duration: TimeInterval = 0
    @Published var volume: Float = 0.9
    @Published var waveformData: [[Float]] = [] // å¤šå£°é“æ³¢å½¢æ•°æ®
    @Published var currentGainDB: Float = 0.0  // å½“å‰AUå¢ç›Šå€¼ï¼ˆç”¨äºUIæ˜¾ç¤ºï¼‰
    @Published var volumeBalanceEnabled = false
    
    private var timer: Timer?
    private var waveformConfig = WaveformProcessingConfig()
    private var currentFileURL: URL?
    private var scheduledStartTime: AVAudioTime?
    
    private init() {
        optimizeWaveformConfig()
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleImportedFile(_:)),
            name: .didImportAudioFile,
            object: nil
        )
    }
    
    private func optimizeWaveformConfig() {
        let coreCount = ProcessInfo.processInfo.processorCount
        let optimalSlices = min(max(coreCount * 2, 4), 16)
        waveformConfig = WaveformProcessingConfig(
            numberOfSlices: optimalSlices,
            targetDataPoints: waveformConfig.targetDataPoints,
            smallFileThreshold: waveformConfig.smallFileThreshold,
            useParallelProcessing: waveformConfig.useParallelProcessing
        )
        print("âœ“ æ³¢å½¢å¤„ç†ä¼˜åŒ– - CPUæ ¸å¿ƒæ•°: \(coreCount), ä¼˜åŒ–åˆ†ç‰‡æ•°: \(optimalSlices)")
    }
    
    @objc private func handleImportedFile(_ note: Notification) {
        guard let fileURL = note.userInfo?["url"] as? URL else { return }
        Task {
            await loadFile(url: fileURL)
        }
    }
    
    func loadFile(url: URL) async {
        stop()
        currentFileURL = url
        
        do {
            // ç»Ÿä¸€ä½¿ç”¨AVAudioEngine
            let engine = AVAudioEngine()
            let playerNode = AVAudioPlayerNode()
            
            // åŠ è½½éŸ³é¢‘æ–‡ä»¶
            let file = try AVAudioFile(forReading: url)
            format = file.processingFormat
            sampleRate = format?.sampleRate ?? 44100
            duration = Double(file.length) / sampleRate
            
            // è®¾ç½®éŸ³é¢‘èŠ‚ç‚¹
            engine.attach(playerNode)
            
            // å¦‚æœéœ€è¦éŸ³é‡å¹³è¡¡ï¼Œæ·»åŠ EQæ•ˆæœå™¨
            if volumeBalanceEnabled && !gains.isEmpty {
                let eq = AVAudioUnitEQ(numberOfBands: 1)
                eq.bands[0].frequency = 1000.0
                eq.bands[0].bandwidth = 1.0
                eq.bands[0].gain = 0.0
                eq.bands[0].bypass = false
                engine.attach(eq)
                eqUnit = eq
                
                // è¿æ¥ï¼šPlayerNode -> EQ -> Output
                engine.connect(playerNode, to: eq, format: format)
                engine.connect(eq, to: engine.mainMixerNode, format: format)
            } else {
                // è¿æ¥ï¼šPlayerNode -> Output
                engine.connect(playerNode, to: engine.mainMixerNode, format: format)
                eqUnit = nil
            }
            
            // è®¾ç½®éŸ³é‡
            engine.mainMixerNode.volume = volume
            
            // å¯åŠ¨å¼•æ“
            try engine.start()
            
            self.audioEngine = engine
            self.playerNode = playerNode
            self.audioFile = file
            
            DispatchQueue.main.async {
                self.isPlaying = false
                self.currentTime = 0
                print("âœ“ éŸ³é¢‘åŠ è½½æˆåŠŸ: \(url.lastPathComponent), æ—¶é•¿: \(String(format: "%.2f", self.duration))s")
            }
        } catch {
            print("âŒ éŸ³é¢‘åŠ è½½å¤±è´¥: \(error.localizedDescription)")
            DispatchQueue.main.async {
                self.duration = 0
                self.isPlaying = false
                self.currentTime = 0
            }
        }
        
        // å¼‚æ­¥ç”Ÿæˆæ³¢å½¢æ•°æ®
        extractWaveformData(from: url)
    }
    
    // è®¾ç½®éŸ³é‡å¹³è¡¡å¢ç›Šæ•°ç»„
    func setVolumeBalanceGains(_ gains: [Float], hopSize: Int = 768) {
        self.gains = gains
        self.hopSize = hopSize
        
        // å¦‚æœæ•ˆæœå™¨å·²å¯ç”¨ï¼Œæ›´æ–°å½“å‰å¢ç›Š
        if volumeBalanceEnabled {
            updateGain(for: currentTime)
        }
        
        // å¦‚æœæ–‡ä»¶å·²åŠ è½½ä¸”æ•ˆæœå™¨å·²å¯ç”¨ï¼Œéœ€è¦é‡æ–°è¿æ¥ä»¥åº”ç”¨æ•ˆæœå™¨
        if let engine = audioEngine,
           let playerNode = playerNode,
           let format = format,
           volumeBalanceEnabled,
           !gains.isEmpty,
           eqUnit == nil {
            // æ•ˆæœå™¨æœªæ·»åŠ ï¼Œéœ€è¦æ·»åŠ 
            let wasPlaying = isPlaying
            let savedTime = currentTime
            
            playerNode.stop()
            
            let eq = AVAudioUnitEQ(numberOfBands: 1)
            eq.bands[0].frequency = 1000.0
            eq.bands[0].bandwidth = 1.0
            eq.bands[0].gain = 0.0
            eq.bands[0].bypass = false
            engine.attach(eq)
            eqUnit = eq
            
            // æ–­å¼€ç°æœ‰è¿æ¥
            engine.disconnectNodeInput(playerNode)
            
            // è¿æ¥ï¼šPlayerNode -> EQ -> Output
            engine.connect(playerNode, to: eq, format: format)
            engine.connect(eq, to: engine.mainMixerNode, format: format)
            
            updateGain(for: savedTime)
            
            // æ¢å¤æ–‡ä»¶ä½ç½®
            if let audioFile = audioFile {
                let framePosition = AVAudioFramePosition(savedTime * sampleRate)
                audioFile.framePosition = framePosition
                
                DispatchQueue.main.async {
                    self.currentTime = savedTime
                    
                    if wasPlaying {
                        playerNode.scheduleFile(audioFile, at: nil) { [weak self] in
                            DispatchQueue.main.async {
                                self?.stop()
                            }
                        }
                        playerNode.play()
                        self.scheduledStartTime = engine.outputNode.lastRenderTime
                        self.isPlaying = true
                        self.startTimer()
                    }
                }
            }
        }
    }
    
    // å¯ç”¨/ç¦ç”¨éŸ³é‡å¹³è¡¡æ•ˆæœå™¨
    func setVolumeBalanceEnabled(_ enabled: Bool) {
        guard volumeBalanceEnabled != enabled else { return }
        
        guard let engine = audioEngine,
              let playerNode = playerNode,
              let audioFile = audioFile,
              let format = format else {
            // å¦‚æœæ–‡ä»¶æœªåŠ è½½ï¼Œåªæ›´æ–°çŠ¶æ€
            volumeBalanceEnabled = enabled
            return
        }
        
        let wasPlaying = isPlaying
        let savedTime = currentTime
        
        // åœæ­¢å½“å‰æ’­æ”¾
        playerNode.stop()
        
        // ç§»é™¤ç°æœ‰è¿æ¥
        engine.disconnectNodeInput(playerNode)
        if let eq = eqUnit {
            engine.disconnectNodeInput(eq)
            engine.detach(eq)
        }
        
        volumeBalanceEnabled = enabled
        
        // å¦‚æœéœ€è¦éŸ³é‡å¹³è¡¡ä¸”æœ‰å¢ç›Šæ•°æ®ï¼Œæ·»åŠ EQæ•ˆæœå™¨
        if enabled && !gains.isEmpty {
            let eq = AVAudioUnitEQ(numberOfBands: 1)
            eq.bands[0].frequency = 1000.0
            eq.bands[0].bandwidth = 1.0
            eq.bands[0].gain = 0.0
            eq.bands[0].bypass = false
            engine.attach(eq)
            eqUnit = eq
            
            // è¿æ¥ï¼šPlayerNode -> EQ -> Output
            engine.connect(playerNode, to: eq, format: format)
            engine.connect(eq, to: engine.mainMixerNode, format: format)
            
            // æ›´æ–°å½“å‰å¢ç›Š
            updateGain(for: savedTime)
        } else {
            // è¿æ¥ï¼šPlayerNode -> Output
            engine.connect(playerNode, to: engine.mainMixerNode, format: format)
            eqUnit = nil
            currentGainDB = 0.0
        }
        
        // æ¢å¤æ–‡ä»¶ä½ç½®å’Œæ’­æ”¾çŠ¶æ€
        let framePosition = AVAudioFramePosition(savedTime * sampleRate)
        audioFile.framePosition = framePosition
        
        DispatchQueue.main.async {
            self.currentTime = savedTime
            
            if wasPlaying {
                // é‡æ–°è°ƒåº¦æ’­æ”¾
                playerNode.scheduleFile(audioFile, at: nil) { [weak self] in
                    DispatchQueue.main.async {
                        self?.stop()
                    }
                }
                
                playerNode.play()
                self.scheduledStartTime = engine.outputNode.lastRenderTime
                self.isPlaying = true
                self.startTimer()
            }
        }
        
        print("ğŸ”Š éŸ³é‡åŠ¨æ€å¹³è¡¡: \(enabled ? "å¯ç”¨" : "ç¦ç”¨")")
    }
    
    func play() {
        guard let playerNode = playerNode,
              let audioFile = audioFile,
              let engine = audioEngine,
              engine.isRunning else {
            print("âš ï¸ æœªåŠ è½½éŸ³é¢‘æ–‡ä»¶æˆ–å¼•æ“æœªå¯åŠ¨")
            return
        }
        
        // å¦‚æœå·²ç»åœ¨æ’­æ”¾ï¼Œä¸åšä»»ä½•æ“ä½œ
        if isPlaying && playerNode.isPlaying {
            return
        }
        
        // åœæ­¢ä¹‹å‰çš„æ’­æ”¾ï¼Œç¡®ä¿æ²¡æœ‰é‡å¤è°ƒåº¦
        if playerNode.isPlaying || scheduledStartTime != nil {
            playerNode.stop()
        }
        
        // ç­‰å¾…èŠ‚ç‚¹å®Œå…¨åœæ­¢
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.01) { [weak self] in
            guard let self = self,
                  let playerNode = self.playerNode,
                  let audioFile = self.audioFile,
                  let engine = self.audioEngine else { return }
            
            // è®¾ç½®æ–‡ä»¶ä½ç½®
            let framePosition = AVAudioFramePosition(self.currentTime * self.sampleRate)
            audioFile.framePosition = framePosition
            
            // è°ƒåº¦æ’­æ”¾
            playerNode.scheduleFile(audioFile, at: nil) { [weak self] in
                DispatchQueue.main.async {
                    self?.stop()
                }
            }
            
            playerNode.play()
            
            self.scheduledStartTime = engine.outputNode.lastRenderTime
            self.isPlaying = true
            self.startTimer()
            
            print("â–¶ï¸ å¼€å§‹æ’­æ”¾")
        }
    }
    
    func pause() {
        guard let playerNode = playerNode else { return }
        
        // ä¿å­˜å½“å‰æ’­æ”¾æ—¶é—´
        if let startTime = scheduledStartTime,
           let engine = audioEngine,
           let playerTime = playerNode.playerTime(forNodeTime: engine.outputNode.lastRenderTime ?? AVAudioTime()) {
            let elapsed = Double(playerTime.sampleTime) / sampleRate
            currentTime = max(0, min(duration, currentTime + elapsed))
        }
        
        playerNode.pause()
        
        DispatchQueue.main.async {
            self.isPlaying = false
            self.stopTimer()
            self.scheduledStartTime = nil
        }
        
        print("â¸ï¸ æš‚åœæ’­æ”¾")
    }
    
    func stop() {
        guard let playerNode = playerNode else { return }
        
        playerNode.stop()
        audioFile?.framePosition = 0
        
        DispatchQueue.main.async {
            self.isPlaying = false
            self.currentTime = 0
            self.stopTimer()
            self.scheduledStartTime = nil
        }
        
        print("â¹ï¸ åœæ­¢æ’­æ”¾")
    }
    
    func seek(to time: TimeInterval) {
        guard let playerNode = playerNode,
              let audioFile = audioFile else { return }
        
        let wasPlaying = isPlaying
        let clampedTime = max(0, min(duration, time))
        
        // å¿…é¡»åœæ­¢æ’­æ”¾ï¼Œé¿å…é‡å¤è°ƒåº¦
        playerNode.stop()
        
        // ç­‰å¾…èŠ‚ç‚¹å®Œå…¨åœæ­¢
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.01) { [weak self] in
            guard let self = self,
                  let playerNode = self.playerNode,
                  let audioFile = self.audioFile else { return }
            
            // è®¾ç½®æ–‡ä»¶ä½ç½®
            let framePosition = AVAudioFramePosition(clampedTime * self.sampleRate)
            audioFile.framePosition = framePosition
            
            self.currentTime = clampedTime
            self.updateGain(for: clampedTime)
            
            // å¦‚æœä¹‹å‰åœ¨æ’­æ”¾ï¼Œç»§ç»­æ’­æ”¾
            if wasPlaying {
                playerNode.scheduleFile(audioFile, at: nil) { [weak self] in
                    DispatchQueue.main.async {
                        self?.stop()
                    }
                }
                
                if let engine = self.audioEngine {
                    self.scheduledStartTime = engine.outputNode.lastRenderTime
                }
                
                playerNode.play()
                self.isPlaying = true
                self.startTimer()
            }
        }
        
        print("â© è·³è½¬åˆ° \(String(format: "%.2f", clampedTime))s")
    }
    
    func setVolume(_ value: Float) {
        volume = max(0, min(value, 1))
        audioEngine?.mainMixerNode.volume = volume
    }
    
    // æ›´æ–°å½“å‰æ—¶é—´çš„å¢ç›Š
    private func updateGain(for time: TimeInterval) {
        guard volumeBalanceEnabled, let eqUnit = eqUnit, !gains.isEmpty else {
            currentGainDB = 0.0
            return
        }
        
        // è®¡ç®—å¯¹åº”çš„å¸§ç´¢å¼•
        let sampleIdx = Int(time * sampleRate)
        let frameIdx = sampleIdx / hopSize
        let gainIdx = min(frameIdx, gains.count - 1)
        
        let gainDB = gains[gainIdx]
        
        // åº”ç”¨å¢ç›Šåˆ°EQé¢‘æ®µ
        eqUnit.bands[0].gain = gainDB
        currentGainDB = gainDB
    }
    
    // MARK: - Timer æ›´æ–°
    private func startTimer() {
        stopTimer()
        // æé«˜æ›´æ–°é¢‘ç‡åˆ° ~60fps (16.67ms) ä»¥è·å¾—å¹³æ»‘çš„æ’­æ”¾æ¡ç§»åŠ¨ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜å€ç¼©æ”¾ä¸‹
        timer = Timer(timeInterval: 1.0 / 60.0, repeats: true) { [weak self] _ in
            self?.updateCurrentTime()
        }
        if let timer = timer {
            RunLoop.main.add(timer, forMode: .common)
        }
    }
    
    private func stopTimer() {
        timer?.invalidate()
        timer = nil
    }
    
    private func updateCurrentTime() {
        guard let playerNode = playerNode,
              let engine = audioEngine,
              isPlaying else { return }
        
        // è®¡ç®—å½“å‰æ’­æ”¾æ—¶é—´
        if let startTime = scheduledStartTime,
           let playerTime = playerNode.playerTime(forNodeTime: engine.outputNode.lastRenderTime ?? AVAudioTime()) {
            let elapsed = Double(playerTime.sampleTime) / sampleRate
            let newTime = max(0, min(duration, currentTime + elapsed))
            
            DispatchQueue.main.async {
                self.currentTime = newTime
                self.updateGain(for: newTime)
                
                // æ£€æŸ¥æ˜¯å¦æ’­æ”¾å®Œæˆ
                if newTime >= self.duration {
                    self.stop()
                }
            }
        } else {
            // å¦‚æœæ— æ³•è·å–ç²¾ç¡®æ—¶é—´ï¼Œä½¿ç”¨ç®€å•ç´¯åŠ 
            DispatchQueue.main.async {
                let newTime = self.currentTime + 1.0 / 60.0
                if newTime >= self.duration {
                    self.stop()
                } else {
                    self.currentTime = newTime
                    self.updateGain(for: newTime)
                }
            }
        }
    }
    
    // MARK: - æ³¢å½¢ç”Ÿæˆï¼ˆå‚è€ƒ Miniwave å¹¶è¡Œå¤„ç†ï¼‰
    private func extractWaveformData(from url: URL) {
        print("ğŸŒŠ å¼€å§‹ç”Ÿæˆæ³¢å½¢æ•°æ®")
        let startTime = CFAbsoluteTimeGetCurrent()
        
        guard let audioFile = try? AVAudioFile(forReading: url) else {
            print("âŒ æ— æ³•æ‰“å¼€éŸ³é¢‘æ–‡ä»¶ç”¨äºæ³¢å½¢ç”Ÿæˆ")
            return
        }
        
        let format = audioFile.processingFormat
        let totalFrameCount = Int(audioFile.length)
        
        // æ ¹æ®é…ç½®å†³å®šå¤„ç†ç­–ç•¥
        if !waveformConfig.useParallelProcessing || totalFrameCount < waveformConfig.smallFileThreshold {
            print("ä½¿ç”¨å•çº¿ç¨‹æ³¢å½¢å¤„ç†")
            extractWaveformDataSingleThread(from: url, audioFile: audioFile, startTime: startTime)
        } else {
            print("ä½¿ç”¨å¹¶è¡Œæ³¢å½¢å¤„ç†")
            extractWaveformDataParallel(from: url, audioFile: audioFile, startTime: startTime)
        }
    }
    
    private func extractWaveformDataSingleThread(from url: URL, audioFile: AVAudioFile, startTime: CFAbsoluteTime) {
        let format = audioFile.processingFormat
        let frameCount = UInt32(audioFile.length)
        
        guard let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameCount) else {
            print("âŒ æ— æ³•åˆ›å»ºéŸ³é¢‘ç¼“å†²åŒº")
            return
        }
        
        do {
            try audioFile.read(into: buffer)
            let sampleCount = Int(buffer.frameLength)
            let samplesPerPixel = max(sampleCount / waveformConfig.targetDataPoints, 1)
            var newWaveformData: [[Float]] = Array(repeating: [], count: Int(format.channelCount))
            
            for channel in 0..<Int(format.channelCount) {
                guard let channelData = buffer.floatChannelData?[channel] else { continue }
                for i in stride(from: 0, to: sampleCount, by: samplesPerPixel) {
                    let segment = Array(UnsafeBufferPointer(start: channelData + i, count: min(samplesPerPixel, sampleCount - i)))
                    let amplitude = segment.map { abs($0) }.max() ?? 0
                    newWaveformData[channel].append(amplitude)
                }
            }
            
            let processingTime = CFAbsoluteTimeGetCurrent() - startTime
            DispatchQueue.main.async {
                self.waveformData = newWaveformData
                print("âœ“ æ³¢å½¢ç”Ÿæˆå®Œæˆ (å•çº¿ç¨‹): \(newWaveformData.first?.count ?? 0)ä¸ªæ•°æ®ç‚¹, è€—æ—¶: \(String(format: "%.3f", processingTime))ç§’")
            }
        } catch {
            print("âŒ è¯»å–éŸ³é¢‘å¤±è´¥: \(error.localizedDescription)")
        }
    }
    
    private func extractWaveformDataParallel(from url: URL, audioFile: AVAudioFile, startTime: CFAbsoluteTime) {
        let format = audioFile.processingFormat
        let totalFrameCount = Int(audioFile.length)
        let channelCount = Int(format.channelCount)
        let numberOfSlices = waveformConfig.numberOfSlices
        let framesPerSlice = totalFrameCount / numberOfSlices
        let samplesPerPixelPerSlice = max(framesPerSlice / (waveformConfig.targetDataPoints / numberOfSlices), 1)
        
        var sliceResults: [[[Float]]] = Array(repeating: Array(repeating: [], count: channelCount), count: numberOfSlices)
        let dispatchGroup = DispatchGroup()
        let resultQueue = DispatchQueue(label: "waveform.result", attributes: .concurrent)
        
        for sliceIndex in 0..<numberOfSlices {
            dispatchGroup.enter()
            DispatchQueue.global(qos: .userInitiated).async {
                defer { dispatchGroup.leave() }
                do {
                    let sliceAudioFile = try AVAudioFile(forReading: url)
                    let startFrame = sliceIndex * framesPerSlice
                    let endFrame = (sliceIndex == numberOfSlices - 1) ? totalFrameCount : (startFrame + framesPerSlice)
                    let actualFrameCount = endFrame - startFrame
                    
                    sliceAudioFile.framePosition = AVAudioFramePosition(startFrame)
                    guard let sliceBuffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: UInt32(actualFrameCount)) else { return }
                    try sliceAudioFile.read(into: sliceBuffer, frameCount: UInt32(actualFrameCount))
                    
                    var sliceWaveformData: [[Float]] = Array(repeating: [], count: channelCount)
                    let sliceSampleCount = Int(sliceBuffer.frameLength)
                    
                    for channel in 0..<channelCount {
                        guard let channelData = sliceBuffer.floatChannelData?[channel] else { continue }
                        for i in stride(from: 0, to: sliceSampleCount, by: samplesPerPixelPerSlice) {
                            let segmentLength = min(samplesPerPixelPerSlice, sliceSampleCount - i)
                            let segment = Array(UnsafeBufferPointer(start: channelData + i, count: segmentLength))
                            let amplitude = segment.map { abs($0) }.max() ?? 0
                            sliceWaveformData[channel].append(amplitude)
                        }
                    }
                    
                    resultQueue.async(flags: .barrier) {
                        sliceResults[sliceIndex] = sliceWaveformData
                    }
                } catch {
                    print("âŒ ç‰‡æ®µ\(sliceIndex)å¤„ç†å¤±è´¥")
                }
            }
        }
        
        dispatchGroup.notify(queue: .global(qos: .userInitiated)) { [weak self] in
            var finalWaveformData: [[Float]] = Array(repeating: [], count: channelCount)
            for channel in 0..<channelCount {
                for sliceIndex in 0..<numberOfSlices {
                    finalWaveformData[channel].append(contentsOf: sliceResults[sliceIndex][channel])
                }
            }
            
            let processingTime = CFAbsoluteTimeGetCurrent() - startTime
            DispatchQueue.main.async {
                self?.waveformData = finalWaveformData
                print("âœ“ æ³¢å½¢ç”Ÿæˆå®Œæˆ (å¹¶è¡Œ): \(finalWaveformData.first?.count ?? 0)ä¸ªæ•°æ®ç‚¹, è€—æ—¶: \(String(format: "%.3f", processingTime))ç§’")
            }
        }
    }
}

