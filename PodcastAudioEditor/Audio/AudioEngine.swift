 import AVFoundation
import Foundation

// æ³¢å½¢å¤„ç†é…ç½®ï¼ˆå‚è€ƒ Miniwaveï¼‰
struct WaveformProcessingConfig {
    let numberOfSlices: Int
    let targetDataPoints: Int
    let smallFileThreshold: Int
    let useParallelProcessing: Bool

    init(numberOfSlices: Int = 10,
         targetDataPoints: Int = 2000,
         smallFileThreshold: Int = 100000,
         useParallelProcessing: Bool = true) {
        self.numberOfSlices = numberOfSlices
        self.targetDataPoints = targetDataPoints
        self.smallFileThreshold = smallFileThreshold
        self.useParallelProcessing = useParallelProcessing
    }
}

final class AudioEngine: ObservableObject {
    static let shared = AudioEngine()

    // AVAudioEngine å’Œæ’­æ”¾èŠ‚ç‚¹
    private var engine: AVAudioEngine?
    private var playerNode: AVAudioPlayerNode?
    private var audioFile: AVAudioFile?
    private var sampleRate: Double = 44100

    // æ•ˆæœå™¨é“¾ï¼ˆ4ä¸ªæ’æ§½ï¼‰
    @Published var effectChain = AudioEffectChain()

    @Published var isPlaying: Bool = false
    @Published var currentTime: TimeInterval = 0
    @Published var duration: TimeInterval = 0
    @Published var volume: Float = 0.9
    @Published var waveformData: [[Float]] = [] // å¤šå£°é“æ³¢å½¢æ•°æ®

    private var timer: Timer?
    private var waveformConfig = WaveformProcessingConfig()
    private var hasScheduledSegment = false  // è¿½è¸ªæ˜¯å¦å·²è°ƒåº¦éŸ³é¢‘æ®µ
    private var currentSegmentID = UUID()    // å½“å‰æ®µçš„å”¯ä¸€æ ‡è¯†ï¼Œç”¨äºå–æ¶ˆæ—§æ®µçš„å›è°ƒ
    private var currentSegmentStartFrame: AVAudioFramePosition = 0  // å½“å‰æ®µçš„èµ·å§‹å¸§

    private init() {
        optimizeWaveformConfig()
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleImportedFile(_:)),
            name: .didImportAudioFile,
            object: nil
        )
        // è®¾ç½®æ•ˆæœé“¾å˜æ›´å›è°ƒ
        effectChain.onEffectChainChanged = { [weak self] in
            self?.reconnectEffectChain()
        }
    }

    private func optimizeWaveformConfig() {
        let coreCount = ProcessInfo.processInfo.processorCount
        let optimalSlices = min(max(coreCount * 2, 4), 16)
        waveformConfig = WaveformProcessingConfig(
            numberOfSlices: optimalSlices,
            targetDataPoints: waveformConfig.targetDataPoints,
            smallFileThreshold: waveformConfig.smallFileThreshold,
            useParallelProcessing: waveformConfig.useParallelProcessing
        )
        print("âœ“ æ³¢å½¢å¤„ç†ä¼˜åŒ– - CPUæ ¸å¿ƒæ•°: \(coreCount), ä¼˜åŒ–åˆ†ç‰‡æ•°: \(optimalSlices)")
    }

    @objc private func handleImportedFile(_ note: Notification) {
        guard let fileURL = note.userInfo?["url"] as? URL else { return }
        Task {
            await loadFile(url: fileURL)
        }
    }

    func loadFile(url: URL) async {
        stop()

        // ç»™æ—§å¼•æ“ä¸€äº›æ—¶é—´æ¥å®Œå…¨æ¸…ç†
        try? await Task.sleep(nanoseconds: 100_000_000) // 100ms

        do {
            // åˆ›å»ºæ–°å¼•æ“
            let newEngine = AVAudioEngine()
            let newPlayerNode = AVAudioPlayerNode()

            // åˆ›å»ºéŸ³é¢‘æ–‡ä»¶
            let newAudioFile = try AVAudioFile(forReading: url)
            let format = newAudioFile.processingFormat
            sampleRate = format.sampleRate

            // é™„åŠ æ’­æ”¾å™¨èŠ‚ç‚¹
            newEngine.attach(newPlayerNode)

            // è·å–åº”è¯¥å¯ç”¨çš„æ•ˆæœå™¨ï¼ˆè€ƒè™‘å…¨å±€å¼€å…³å’Œæ’æ§½å¼€å…³ï¼‰
            let effectsToAttach: [AVAudioUnit]
            if effectChain.isEnabled {
                effectsToAttach = effectChain.getEnabledAudioUnits()
                print("ğŸ“¦ æ•ˆæœé“¾å¯ç”¨ï¼Œé™„åŠ  \(effectsToAttach.count) ä¸ªå·²å¯ç”¨çš„æ•ˆæœå™¨")
            } else {
                effectsToAttach = []
                print("ğŸ“¦ æ•ˆæœé“¾ç¦ç”¨ï¼Œä¸é™„åŠ æ•ˆæœå™¨")
            }

            // é™„åŠ æ•ˆæœå™¨èŠ‚ç‚¹
            for unit in effectsToAttach {
                newEngine.attach(unit)
            }

            // æ„å»ºè¿æ¥ï¼šPlayerNode -> Effect1 -> Effect2 -> Effect3 -> Effect4 -> MainMixer -> Output
            var previousNode: AVAudioNode = newPlayerNode

            for unit in effectsToAttach {
                newEngine.connect(previousNode, to: unit, format: format)
                previousNode = unit
            }

            // è¿æ¥æœ€åä¸€ä¸ªèŠ‚ç‚¹åˆ°ä¸»æ··éŸ³å™¨
            newEngine.connect(previousNode, to: newEngine.mainMixerNode, format: format)

            // å‡†å¤‡å¼•æ“
            try newEngine.prepare()

            // ä¿å­˜å¼•ç”¨
            self.engine = newEngine
            self.playerNode = newPlayerNode
            self.audioFile = newAudioFile
            self.hasScheduledSegment = false  // é‡ç½®è°ƒåº¦æ ‡å¿—

            DispatchQueue.main.async {
                self.duration = Double(newAudioFile.length) / self.sampleRate
                self.isPlaying = false
                self.currentTime = 0
                print("âœ“ éŸ³é¢‘åŠ è½½æˆåŠŸ: \(url.lastPathComponent), æ—¶é•¿: \(String(format: "%.2f", self.duration))s")
                self.effectChain.printChainStatus()
            }

            // å¼‚æ­¥ç”Ÿæˆæ³¢å½¢æ•°æ®
            extractWaveformData(from: url)
        } catch {
            print("âŒ éŸ³é¢‘åŠ è½½å¤±è´¥: \(error.localizedDescription)")
        }
    }

    func play() {
        guard let playerNode = playerNode,
              let audioFile = audioFile,
              let engine = engine else {
            print("âš ï¸ æœªåŠ è½½éŸ³é¢‘æ–‡ä»¶")
            return
        }

        print("ğŸ¬ play() è°ƒç”¨ - currentTime: \(currentTime), hasScheduledSegment: \(hasScheduledSegment), isPlaying: \(isPlaying)")

        // å¯åŠ¨å¼•æ“
        if !engine.isRunning {
            do {
                try engine.start()
                print("âœ… å¼•æ“å·²å¯åŠ¨")
            } catch {
                print("âŒ å¯åŠ¨å¼•æ“å¤±è´¥: \(error.localizedDescription)")
                return
            }
        }

        // åªåœ¨æ²¡æœ‰è°ƒåº¦è¿‡æ®µçš„æƒ…å†µä¸‹æ‰è°ƒåº¦
        if !hasScheduledSegment {
            print("ğŸ“… è°ƒåº¦éŸ³é¢‘ä» \(currentTime)s å¼€å§‹")
            scheduleAudioFile(at: currentTime)
            hasScheduledSegment = true
        } else {
            print("â­ï¸ è·³è¿‡è°ƒåº¦ï¼ˆå·²ç»è°ƒåº¦è¿‡ï¼‰")
        }

        playerNode.play()

        DispatchQueue.main.async {
            self.isPlaying = true
            self.startTimer()
        }
        print("â–¶ï¸ æ’­æ”¾èŠ‚ç‚¹å·²å¯åŠ¨")
    }

    func pause() {
        print("â¸ï¸ pause() è°ƒç”¨ - currentTime: \(currentTime)")
        playerNode?.pause()
        // æ³¨æ„ï¼šæš‚åœæ—¶ä¸é‡ç½® hasScheduledSegmentï¼Œå› ä¸ºæ®µä»ç„¶æœ‰æ•ˆ
        // åªæœ‰ stop() æˆ– seek() æ‰éœ€è¦é‡ç½®è°ƒåº¦çŠ¶æ€
        DispatchQueue.main.async {
            self.isPlaying = false
            self.stopTimer()
        }
        print("â¸ï¸ æš‚åœå®Œæˆ - hasScheduledSegmentä¿æŒä¸º: \(hasScheduledSegment)")
    }

    func stop() {
        print("â¹ï¸ stop() è°ƒç”¨")
        playerNode?.stop()
        engine?.stop()
        hasScheduledSegment = false  // é‡ç½®è°ƒåº¦æ ‡å¿—

        DispatchQueue.main.async {
            self.isPlaying = false
            self.currentTime = 0
            self.stopTimer()
        }
        print("â¹ï¸ åœæ­¢å®Œæˆ")
    }

    func seek(to time: TimeInterval) {
        guard let playerNode = playerNode,
              let audioFile = audioFile,
              let engine = engine else { return }

        let wasPlaying = isPlaying

        // ç¡®ä¿æ—¶é—´åœ¨æœ‰æ•ˆèŒƒå›´å†…
        let clampedTime = max(0, min(time, duration))

        print("ğŸ” seek() è°ƒç”¨ - ç›®æ ‡: \(clampedTime)s, wasPlaying: \(wasPlaying), currentTime: \(currentTime)")

        // è®¡ç®—èµ·å§‹å¸§
        let startFrame = AVAudioFramePosition(clampedTime * sampleRate)
        let totalFrames = audioFile.length - startFrame

        print("ğŸ“Š èµ·å§‹å¸§: \(startFrame), æ€»å¸§æ•°: \(totalFrames), æ–‡ä»¶é•¿åº¦: \(audioFile.length)")

        guard totalFrames > 0 && startFrame >= 0 && startFrame < audioFile.length else {
            print("âš ï¸ æ— æ•ˆçš„seekä½ç½®: \(clampedTime)s")
            DispatchQueue.main.async {
                self.currentTime = clampedTime
            }
            return
        }

        // åœæ­¢æ’­æ”¾èŠ‚ç‚¹
        playerNode.stop()
        hasScheduledSegment = false  // é‡ç½®è°ƒåº¦æ ‡å¿—
        currentSegmentID = UUID()     // ç”Ÿæˆæ–°çš„æ®µIDï¼Œä½¿æ—§æ®µå›è°ƒå¤±æ•ˆ
        print("ğŸ›‘ æ’­æ”¾èŠ‚ç‚¹å·²åœæ­¢ï¼ŒhasScheduledSegmenté‡ç½®ä¸ºfalseï¼Œæ–°æ®µID: \(currentSegmentID)")

        // ç¡®ä¿å¼•æ“åœ¨è¿è¡Œ
        if !engine.isRunning {
            do {
                try engine.start()
                print("âœ… å¼•æ“å·²å¯åŠ¨")
            } catch {
                print("âŒ å¯åŠ¨å¼•æ“å¤±è´¥: \(error.localizedDescription)")
                return
            }
        }

        // è°ƒåº¦ä»æ–°ä½ç½®å¼€å§‹çš„éŸ³é¢‘
        let segmentID = currentSegmentID  // æ•è·å½“å‰ID
        currentSegmentStartFrame = startFrame  // ä¿å­˜æ®µèµ·å§‹å¸§
        print("ğŸ“… è°ƒåº¦æ–°æ®µ - èµ·å§‹å¸§: \(startFrame), å¸§æ•°: \(totalFrames), æ®µID: \(segmentID)")
        playerNode.scheduleSegment(audioFile, startingFrame: startFrame, frameCount: AVAudioFrameCount(totalFrames), at: nil) { [weak self] in
            DispatchQueue.main.async {
                guard let self = self else { return }
                // åªæœ‰å½“è¿™æ˜¯å½“å‰æœ‰æ•ˆçš„æ®µæ—¶æ‰æ‰§è¡Œå›è°ƒ
                if self.currentSegmentID == segmentID && self.isPlaying {
                    print("âœ… éŸ³é¢‘æ®µæ’­æ”¾å®Œæˆ (æ®µID: \(segmentID))")
                    self.stop()
                } else {
                    print("â­ï¸ å¿½ç•¥æ—§æ®µå›è°ƒ (æ®µID: \(segmentID), å½“å‰ID: \(self.currentSegmentID))")
                }
            }
        }
        hasScheduledSegment = true  // æ ‡è®°å·²è°ƒåº¦
        print("âœ… æ®µå·²è°ƒåº¦ï¼ŒhasScheduledSegmentè®¾ç½®ä¸ºtrue")

        // ç«‹å³æ›´æ–° currentTimeï¼ˆä¸è¦å¼‚æ­¥ï¼‰
        self.currentTime = clampedTime
        print("â±ï¸ currentTimeç«‹å³æ›´æ–°ä¸º: \(clampedTime)s")

        if wasPlaying {
            // æ¢å¤æ’­æ”¾
            print("â–¶ï¸ æ¢å¤æ’­æ”¾")
            playerNode.play()
            DispatchQueue.main.async {
                self.isPlaying = true
                self.startTimer()
            }
        } else {
            print("â¸ï¸ ä¿æŒæš‚åœçŠ¶æ€")
        }

        print("â© seekå®Œæˆ - è·³è½¬åˆ° \(clampedTime)s")
    }

    // è°ƒåº¦éŸ³é¢‘æ–‡ä»¶æ’­æ”¾
    private func scheduleAudioFile(at startTime: TimeInterval = 0) {
        guard let playerNode = playerNode,
              let audioFile = audioFile else {
            print("âš ï¸ scheduleAudioFile - æ²¡æœ‰playerNodeæˆ–audioFile")
            return
        }

        // ç¡®ä¿æ—¶é—´åœ¨æœ‰æ•ˆèŒƒå›´å†…
        let clampedTime = max(0, min(startTime, duration))
        let startFrame = AVAudioFramePosition(clampedTime * sampleRate)
        let totalFrames = audioFile.length - startFrame

        print("ğŸ“… scheduleAudioFile - å¼€å§‹æ—¶é—´: \(startTime)s, èµ·å§‹å¸§: \(startFrame), å¸§æ•°: \(totalFrames)")

        guard totalFrames > 0 && startFrame >= 0 && startFrame < audioFile.length else {
            print("âš ï¸ scheduleAudioFile - æ— æ•ˆçš„seekä½ç½®: \(startTime)s")
            return
        }

        // ç”Ÿæˆæ–°çš„æ®µID
        currentSegmentID = UUID()
        currentSegmentStartFrame = startFrame  // ä¿å­˜æ®µèµ·å§‹å¸§
        let segmentID = currentSegmentID
        print("ğŸ“… scheduleAudioFile - æ®µID: \(segmentID), èµ·å§‹å¸§: \(startFrame)")

        // ä»æŒ‡å®šä½ç½®è¯»å–å¹¶æ’­æ”¾
        playerNode.scheduleSegment(audioFile, startingFrame: startFrame, frameCount: AVAudioFrameCount(totalFrames), at: nil) { [weak self] in
            DispatchQueue.main.async {
                guard let self = self else { return }
                // åªæœ‰å½“è¿™æ˜¯å½“å‰æœ‰æ•ˆçš„æ®µæ—¶æ‰æ‰§è¡Œå›è°ƒ
                if self.currentSegmentID == segmentID && self.isPlaying {
                    print("âœ… scheduleAudioFileçš„æ®µæ’­æ”¾å®Œæˆ (æ®µID: \(segmentID))")
                    self.stop()
                } else {
                    print("â­ï¸ å¿½ç•¥scheduleAudioFileçš„æ—§æ®µå›è°ƒ (æ®µID: \(segmentID), å½“å‰ID: \(self.currentSegmentID))")
                }
            }
        }
        print("âœ… scheduleAudioFile - æ®µå·²è°ƒåº¦")
    }

    func setVolume(_ value: Float) {
        volume = max(0, min(value, 1))
        engine?.mainMixerNode.volume = volume
    }

    // MARK: - åŠ¨æ€é‡æ–°è¿æ¥æ•ˆæœé“¾
    private func reconnectEffectChain() {
        guard let playerNode = playerNode,
              let audioFile = audioFile,
              let engine = engine else {
            print("âš ï¸ reconnectEffectChain - æ²¡æœ‰å¼•æ“æˆ–éŸ³é¢‘æ–‡ä»¶ï¼Œè·³è¿‡é‡è¿")
            return
        }

        print("ğŸ”„ å¼€å§‹é‡æ–°è¿æ¥æ•ˆæœé“¾ - wasPlaying: \(isPlaying), currentTime: \(currentTime)")

        // ä¿å­˜å½“å‰çŠ¶æ€
        let wasPlaying = isPlaying
        let savedTime = currentTime

        // åœæ­¢æ’­æ”¾èŠ‚ç‚¹
        playerNode.stop()
        hasScheduledSegment = false  // é‡ç½®è°ƒåº¦æ ‡å¿—ï¼Œå› ä¸ºè¦é‡å»ºæ•´ä¸ªå¼•æ“

        // è·å–å½“å‰çš„æ•ˆæœå™¨å•å…ƒï¼ˆåœ¨åœæ­¢å¼•æ“ä¹‹å‰ï¼‰
        let currentEffects = effectChain.getEnabledAudioUnits()
        print("ğŸ”— å‡†å¤‡åˆ†ç¦» \(currentEffects.count) ä¸ªæ•ˆæœå™¨èŠ‚ç‚¹")

        // åœæ­¢å¹¶åˆ†ç¦»æ—§å¼•æ“ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
        if engine.isRunning {
            engine.stop()
        }

        // å…³é”®ï¼šä»æ—§å¼•æ“ä¸­åˆ†ç¦»æ‰€æœ‰ AudioUnit èŠ‚ç‚¹
        for unit in currentEffects {
            print("ğŸ”Œ ä»æ—§å¼•æ“åˆ†ç¦»æ•ˆæœå™¨: \(unit)")
            engine.detach(unit)
        }

        do {
            // åˆ›å»ºä¸€ä¸ªæ–°å¼•æ“å¹¶é‡æ–°é™„åŠ æ‰€æœ‰èŠ‚ç‚¹
            let newEngine = AVAudioEngine()
            let newPlayerNode = AVAudioPlayerNode()

            newEngine.attach(newPlayerNode)

            // è·å–åº”è¯¥å¯ç”¨çš„æ•ˆæœå™¨ï¼ˆè€ƒè™‘å…¨å±€å¼€å…³å’Œæ’æ§½å¼€å…³ï¼‰
            let effectsToAttach: [AVAudioUnit]
            if effectChain.isEnabled {
                effectsToAttach = effectChain.getEnabledAudioUnits()
                print("ğŸ“¦ æ•ˆæœé“¾å¯ç”¨ï¼Œé™„åŠ  \(effectsToAttach.count) ä¸ªå·²å¯ç”¨çš„æ•ˆæœå™¨åˆ°æ–°å¼•æ“")
            } else {
                effectsToAttach = []
                print("ğŸ“¦ æ•ˆæœé“¾ç¦ç”¨ï¼Œä¸é™„åŠ æ•ˆæœå™¨åˆ°æ–°å¼•æ“")
            }

            // é™„åŠ æ•ˆæœå™¨èŠ‚ç‚¹åˆ°æ–°å¼•æ“
            for unit in effectsToAttach {
                print("ğŸ”Œ é™„åŠ æ•ˆæœå™¨åˆ°æ–°å¼•æ“: \(unit)")
                newEngine.attach(unit)
            }

            // é‡æ–°æ„å»ºè¿æ¥
            let format = audioFile.processingFormat
            var previousNode: AVAudioNode = newPlayerNode

            for unit in effectsToAttach {
                newEngine.connect(previousNode, to: unit, format: format)
                previousNode = unit
            }

            // è¿æ¥åˆ°ä¸»æ··éŸ³å™¨
            newEngine.connect(previousNode, to: newEngine.mainMixerNode, format: format)

            // å‡†å¤‡å¹¶å¯åŠ¨æ–°å¼•æ“
            try newEngine.prepare()
            try newEngine.start()

            // æ›¿æ¢å¼•æ“
            self.engine = newEngine
            self.playerNode = newPlayerNode

            print("âœ“ æ•ˆæœé“¾å·²é‡æ–°è¿æ¥ï¼Œå¼•æ“å·²å¯åŠ¨")

            // é‡æ–°è°ƒåº¦éŸ³é¢‘ï¼ˆä»ä¿å­˜çš„æ—¶é—´ä½ç½®å¼€å§‹ï¼‰
            scheduleAudioFile(at: savedTime)
            hasScheduledSegment = true
            print("âœ… å·²é‡æ–°è°ƒåº¦éŸ³é¢‘ä» \(savedTime)s")

            // å¦‚æœä¹‹å‰åœ¨æ’­æ”¾ï¼Œæ¢å¤æ’­æ”¾
            if wasPlaying {
                print("â–¶ï¸ æ¢å¤æ’­æ”¾")
                newPlayerNode.play()
                DispatchQueue.main.async {
                    self.isPlaying = true
                    self.startTimer()
                }
            } else {
                print("â¸ï¸ ä¿æŒæš‚åœçŠ¶æ€ï¼ˆéŸ³é¢‘å·²è°ƒåº¦ï¼‰")
                DispatchQueue.main.async {
                    self.isPlaying = false
                }
            }
        } catch {
            print("âŒ é‡æ–°è¿æ¥æ•ˆæœé“¾å¤±è´¥: \(error.localizedDescription)")
        }
    }

    // MARK: - Timer æ›´æ–°
    private func startTimer() {
        stopTimer()
        timer = Timer(timeInterval: 1.0 / 60.0, repeats: true) { [weak self] _ in
            self?.updateCurrentTime()
        }
        if let timer = timer {
            RunLoop.main.add(timer, forMode: .common)
        }
    }

    private func stopTimer() {
        timer?.invalidate()
        timer = nil
    }

    private func updateCurrentTime() {
        guard let playerNode = playerNode else { return }

        // è®¡ç®—å½“å‰æ’­æ”¾ä½ç½®
        if let nodeTime = playerNode.lastRenderTime,
           let playerTime = playerNode.playerTime(forNodeTime: nodeTime) {
            let sampleTime = Double(playerTime.sampleTime)

            guard sampleTime >= 0 else { return }

            // playerTime.sampleTime æ˜¯ç›¸å¯¹äºæ®µå¼€å§‹çš„æ—¶é—´
            // åŠ ä¸Šæ®µçš„èµ·å§‹å¸§æ¥è·å¾—æ–‡ä»¶ä¸­çš„ç»å¯¹æ—¶é—´
            let absoluteSampleTime = Double(currentSegmentStartFrame) + sampleTime
            let newTime = absoluteSampleTime / sampleRate

            // ç¡®ä¿æ—¶é—´åœ¨æœ‰æ•ˆèŒƒå›´å†…
            let clampedTime = max(0, min(newTime, duration))

            DispatchQueue.main.async {
                self.currentTime = clampedTime

                if !playerNode.isPlaying && self.isPlaying {
                    self.isPlaying = false
                    self.stopTimer()
                }
            }
        }
    }

    // MARK: - åŠ¨æ€éŸ³é‡å¹³è¡¡å¢ç›Šåº”ç”¨
    /// åº”ç”¨å¢ç›ŠåŒ…ç»œåˆ° AUPeakLimiter çš„ Pre-Gain å‚æ•°
    /// - Parameters:
    ///   - gainValue: å¢ç›Šå€¼ï¼ˆdBï¼‰
    func applyDynamicGain(_ gainValue: Float) {
        // è·å–æ’æ§½1çš„AUPeakLimiteræ•ˆæœå™¨
        guard let slot = effectChain.getSlot(0),
              let audioUnit = slot.audioUnit else { return }

        let auAudioUnit = audioUnit.auAudioUnit

        // æŸ¥æ‰¾ Pre-Gain å‚æ•°ï¼ˆID é€šå¸¸æ˜¯ 0ï¼‰
        guard let parameterTree = auAudioUnit.parameterTree else { return }

        // éå†å‚æ•°æ‰¾åˆ° Pre-Gain
        if let preGainParam = parameterTree.allParameters.first(where: { param in
            param.displayName.lowercased().contains("pregain") ||
            param.displayName.lowercased().contains("pre-gain") ||
            param.displayName.lowercased().contains("input")
        }) {
            // å°† gainValueï¼ˆdBï¼‰è½¬æ¢ä¸ºå‚æ•°å€¼
            // Pre-Gain é€šå¸¸ä»¥ dB ä¸ºå•ä½ï¼ŒèŒƒå›´ -96 åˆ° 24
            let clampedGain = max(-96.0, min(24.0, gainValue))
            preGainParam.value = AUValue(clampedGain)

            print("ğŸ”Š åº”ç”¨å¢ç›Š: \(String(format: "%.2f", gainValue))dB åˆ° Pre-Gain")
        }
    }

    // MARK: - æ³¢å½¢ç”Ÿæˆï¼ˆå‚è€ƒ Miniwave å¹¶è¡Œå¤„ç†ï¼‰
    private func extractWaveformData(from url: URL) {
        print("ğŸŒŠ å¼€å§‹ç”Ÿæˆæ³¢å½¢æ•°æ®")
        let startTime = CFAbsoluteTimeGetCurrent()

        guard let audioFile = try? AVAudioFile(forReading: url) else {
            print("âŒ æ— æ³•æ‰“å¼€éŸ³é¢‘æ–‡ä»¶ç”¨äºæ³¢å½¢ç”Ÿæˆ")
            return
        }

        let totalFrameCount = Int(audioFile.length)

        // æ ¹æ®é…ç½®å†³å®šå¤„ç†ç­–ç•¥
        if !waveformConfig.useParallelProcessing || totalFrameCount < waveformConfig.smallFileThreshold {
            print("ä½¿ç”¨å•çº¿ç¨‹æ³¢å½¢å¤„ç†")
            extractWaveformDataSingleThread(from: url, audioFile: audioFile, startTime: startTime)
        } else {
            print("ä½¿ç”¨å¹¶è¡Œæ³¢å½¢å¤„ç†")
            extractWaveformDataParallel(from: url, audioFile: audioFile, startTime: startTime)
        }
    }

    private func extractWaveformDataSingleThread(from url: URL, audioFile: AVAudioFile, startTime: CFAbsoluteTime) {
        let format = audioFile.processingFormat
        let frameCount = UInt32(audioFile.length)

        guard let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: frameCount) else {
            print("âŒ æ— æ³•åˆ›å»ºéŸ³é¢‘ç¼“å†²åŒº")
            return
        }

        do {
            try audioFile.read(into: buffer)
            let sampleCount = Int(buffer.frameLength)
            let samplesPerPixel = max(sampleCount / waveformConfig.targetDataPoints, 1)
            var newWaveformData: [[Float]] = Array(repeating: [], count: Int(format.channelCount))

            for channel in 0..<Int(format.channelCount) {
                guard let channelData = buffer.floatChannelData?[channel] else { continue }
                for i in stride(from: 0, to: sampleCount, by: samplesPerPixel) {
                    let segment = Array(UnsafeBufferPointer(start: channelData + i, count: min(samplesPerPixel, sampleCount - i)))
                    let amplitude = segment.map { abs($0) }.max() ?? 0
                    newWaveformData[channel].append(amplitude)
                }
            }

            let processingTime = CFAbsoluteTimeGetCurrent() - startTime
            DispatchQueue.main.async {
                self.waveformData = newWaveformData
                print("âœ“ æ³¢å½¢ç”Ÿæˆå®Œæˆ (å•çº¿ç¨‹): \(newWaveformData.first?.count ?? 0)ä¸ªæ•°æ®ç‚¹, è€—æ—¶: \(String(format: "%.3f", processingTime))ç§’")
            }
        } catch {
            print("âŒ è¯»å–éŸ³é¢‘å¤±è´¥: \(error.localizedDescription)")
        }
    }

    private func extractWaveformDataParallel(from url: URL, audioFile: AVAudioFile, startTime: CFAbsoluteTime) {
        let format = audioFile.processingFormat
        let totalFrameCount = Int(audioFile.length)
        let channelCount = Int(format.channelCount)
        let numberOfSlices = waveformConfig.numberOfSlices
        let framesPerSlice = totalFrameCount / numberOfSlices
        let samplesPerPixelPerSlice = max(framesPerSlice / (waveformConfig.targetDataPoints / numberOfSlices), 1)

        var sliceResults: [[[Float]]] = Array(repeating: Array(repeating: [], count: channelCount), count: numberOfSlices)
        let dispatchGroup = DispatchGroup()
        let resultQueue = DispatchQueue(label: "waveform.result", attributes: .concurrent)

        for sliceIndex in 0..<numberOfSlices {
            dispatchGroup.enter()
            DispatchQueue.global(qos: .userInitiated).async {
                defer { dispatchGroup.leave() }
                do {
                    let sliceAudioFile = try AVAudioFile(forReading: url)
                    let startFrame = sliceIndex * framesPerSlice
                    let endFrame = (sliceIndex == numberOfSlices - 1) ? totalFrameCount : (startFrame + framesPerSlice)
                    let actualFrameCount = endFrame - startFrame

                    sliceAudioFile.framePosition = AVAudioFramePosition(startFrame)
                    guard let sliceBuffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: UInt32(actualFrameCount)) else { return }
                    try sliceAudioFile.read(into: sliceBuffer, frameCount: UInt32(actualFrameCount))

                    var sliceWaveformData: [[Float]] = Array(repeating: [], count: channelCount)
                    let sliceSampleCount = Int(sliceBuffer.frameLength)

                    for channel in 0..<channelCount {
                        guard let channelData = sliceBuffer.floatChannelData?[channel] else { continue }
                        for i in stride(from: 0, to: sliceSampleCount, by: samplesPerPixelPerSlice) {
                            let segmentLength = min(samplesPerPixelPerSlice, sliceSampleCount - i)
                            let segment = Array(UnsafeBufferPointer(start: channelData + i, count: segmentLength))
                            let amplitude = segment.map { abs($0) }.max() ?? 0
                            sliceWaveformData[channel].append(amplitude)
                        }
                    }

                    resultQueue.async(flags: .barrier) {
                        sliceResults[sliceIndex] = sliceWaveformData
                    }
                } catch {
                    print("âŒ ç‰‡æ®µ\(sliceIndex)å¤„ç†å¤±è´¥")
                }
            }
        }

        dispatchGroup.notify(queue: .global(qos: .userInitiated)) { [weak self] in
            var finalWaveformData: [[Float]] = Array(repeating: [], count: channelCount)
            for channel in 0..<channelCount {
                for sliceIndex in 0..<numberOfSlices {
                    finalWaveformData[channel].append(contentsOf: sliceResults[sliceIndex][channel])
                }
            }

            let processingTime = CFAbsoluteTimeGetCurrent() - startTime
            DispatchQueue.main.async {
                self?.waveformData = finalWaveformData
                print("âœ“ æ³¢å½¢ç”Ÿæˆå®Œæˆ (å¹¶è¡Œ): \(finalWaveformData.first?.count ?? 0)ä¸ªæ•°æ®ç‚¹, è€—æ—¶: \(String(format: "%.3f", processingTime))ç§’")
            }
        }
    }
}
